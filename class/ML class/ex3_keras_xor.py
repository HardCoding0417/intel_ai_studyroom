# -*- coding: utf-8 -*-
"""ex3_keras_xor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IHkzTYFPErw4E2RLGwDL329WJKCXXwcG

#인공지능의 역사
"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

# two inputs
training_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])
# one output
target_data = np.array([ [0], [1], [1], [0] ])

model = Sequential()
# (prof.fitial) 2 layers; add a layer == expand dimension
# Dense: output would be an input for all perceptron of the next layer
# relu: to solve vanishing gradient problem caused by several sigmoid functions
model.add(Dense(32, input_dim=2, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='mse', optimizer='adam')
model.summary()

fit_hist = model.fit(training_data, target_data, epochs=1000, verbose=1)

plt.plot(fit_hist.history['loss'])
plt.show()

print(model.predict(np.array([[0, 0], [0, 1], [1, 0], [1, 1]])).round())